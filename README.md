[微信公众号：NLP PaperWeekly](https://raw.githubusercontent.com/hflyzju/blog-img/main/image-20240511140641350.png)

[知乎：HxShine](https://www.zhihu.com/people/hu-xiang-67-61/posts)

[CSDN：HxShine](https://blog.csdn.net/qq_16949707)

| 类别             | 文章                                                         | 日期 |
| ---------------- | ------------------------------------------------------------ | ---- |
| RAG              | [UC Berkeley ｜RAFT: 对不相关的RAG检索结果进行建模](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484667&idx=1&sn=81e6f62bf42c4e4b51aade5f1a4295a8&chksm=cf2b7e03f85cf7151ed37c381dd49fe648014176b468534d07f86fbfad9306ee6e1102fb9a72&token=347564426&lang=zh_CN#rd) | 2024 |
| RAG              | [百川智能RAG方案总结：搜索出生的百川智能大模型RAG爬坑之路](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484382&idx=1&sn=659b1582ddb11307c9dfc0cee87bb2a3&chksm=cf2b7926f85cf030000d3550b9776c4093f094863b5ae925f9764f714337f484f2f336a4eeba&token=347564426&lang=zh_CN#rd) | 2024 |
| RAG              | [kaggle大模型竞赛优胜方案总结与思考](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484250&idx=1&sn=9c696b30923392a5e99a7648ca0c763b&chksm=cf2b79a2f85cf0b42362f216663e94793a8dc729a8529ea17ca2cf31635903b2af5344afee19&token=347564426&lang=zh_CN#rd) | 2023 |
| RAG              | [LLM4CS：一种利用LLM提升多轮会话检索的效果的方案](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484736&idx=1&sn=31e4a8b8113e31fbee1d9613b3ae48dd&chksm=cf2b7fb8f85cf6aebcdda8be8579bde0ab2546b9c04e52172a49d9600401be9fb7759b9d5d03#rd) | 2024 |
| COT              | [GOOGLE \| COT（chain of thought）开山之作，利用思维链提升复杂问题推理能力](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483895&idx=1&sn=33ab2fe70af404d528f0771ae5416c87&chksm=cf2b7b0ff85cf21928bba2205f9a3b61b44486bda55947f9f6f2891a4bf6d1b3787cfbf523e5&token=347564426&lang=zh_CN#rd) | 2023 |
| COT              | [Google \| HtT: 大模型通过学习推理规则改善幻觉现象](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484287&idx=2&sn=0704d99691a0f9fa9eacfb66faea9583&chksm=cf2b7987f85cf091b6f38dd53fbab51f96f1799a4108e39b34f320e172bb8ea5ea7a0aa02b63&token=347564426&lang=zh_CN#rd) | 2023 |
| 推理能力优化     | [Self-Consistency: Google超简单方法改善大模型推理能力](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483973&idx=1&sn=09c74bf28f60a68612c8ccb3321cc616&chksm=cf2b78bdf85cf1abd5da147ac74ec9591a29ab91adf6ce149d71e97f7106d44085a2abb4195f&token=347564426&lang=zh_CN#rd) | 2023 |
| ICL              | [GOOGLE:只有大模型才能理解你举的例子（In-context learning）是什么](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483761&idx=1&sn=bc4cd2de106578b6781b051a85490d97&chksm=cf2b7b89f85cf29fbae68843a535078ae212ca52c97e04f1dcdaadf917eee89a4fa4ec1b158d&token=347564426&lang=zh_CN#rd) | 2023 |
| ICL              | [ACL2023 : 预训练模型能否对新注入的知识进行推理](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483749&idx=1&sn=c3a9b21f4137e03ffeebce48070a5596&chksm=cf2b7b9df85cf28b8b4322ec8146846c2eac07e57f61a09e6b636887e842e175c4739421b46a&token=347564426&lang=zh_CN#rd) | 2023 |
| ICL              | [ACL2023 \| Self-Adaptive ICL: 完蛋！我被高质量示例包围了！](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484337&idx=3&sn=789ebdcf5b6839d4cdd7347f7f0bddbe&chksm=cf2b7949f85cf05ff6d994bf91359b3a1ff82dee29923a72b18665f42105b89f7715b6cf36a2&token=347564426&lang=zh_CN#rd) | 2024 |
| LLM技术报告      | [Gemini技术报告解读](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484364&idx=1&sn=e5a7647c65f41f420b78c343b3fbb19d&chksm=cf2b7934f85cf0221029ef1cf4311fda388117baf901a41484de0921291100fe49a13d408d5f&token=347564426&lang=zh_CN#rd) | 2023 |
| LLM技术报告      | [LLama1技术报告解读](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484143&idx=1&sn=5379d35aac6f973d0af84e4a00711b9a&chksm=cf2b7817f85cf101e84ad8fdaad2629bae384ba0bf31adf3686603fffec586defcf95d232c9a&token=347564426&lang=zh_CN#rd) | 2023 |
| LLM技术报告      | [Meta开源之光LLama2是如何追上ChatGPT的？](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484191&idx=1&sn=6a27e9448192b42fff2ed78a6ed526b1&chksm=cf2b79e7f85cf0f1765cacb6fdf84c13822ecd8cfaf8537907b6eafdd9dec7ca38395b295f50&token=347564426&lang=zh_CN#rd) | 2023 |
| 模型蒸馏         | [ACL2023 ｜小模型也能一步步“思考”](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484287&idx=1&sn=f5bd6b334529ee20776dff777d215a36&chksm=cf2b7987f85cf0913fba3a943548cad80181b545853779130a830e599b62312f3e673cb4b0c1&token=347564426&lang=zh_CN#rd) | 2023 |
| SFT数据构建      | [ACL2023 \| 大模型如何快速构建指令遵循数据集？Self-Instruct：只需175条种子数据追上InstructGPT](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483933&idx=1&sn=b57e449230e1548b432fa43efe388c7c&chksm=cf2b78e5f85cf1f31d0f915ae08e07fd454724d9a6a11aea2bd8fb0e8921944990229f369f97&token=347564426&lang=zh_CN#rd) | 2023 |
| SFT数据构建      | [指令回译：如何从大量无标签文档挖掘高质量大模型训练数据？](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484212&idx=1&sn=78d38ec9feb47289d7cf6816871db5ef&chksm=cf2b79ccf85cf0da6787d22487dd885e6f5030f6e2913bc948b14a3a9ecbe9a76ee8d94cf939&token=347564426&lang=zh_CN#rd) | 2023 |
| SFT微调          | [清华p-tuning解决GPT系列模型fine-tuning效果比BERT差问题](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483801&idx=1&sn=e42f1fa8233aca3fa03af0a44bb73f74&chksm=cf2b7b61f85cf27795385e70b53c1a694dd4ae50ead0912776de0cbbc75bb64d210bee3bc5a8&token=347564426&lang=zh_CN#rd) | 2023 |
| SFT微调          | [chatglm微调神器P-Tuning v2论文学习](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483771&idx=1&sn=c216b43c3f3716e78f196fd1c579dc68&chksm=cf2b7b83f85cf295cb6be3ddc975c5b6a269bac6b78a3e549b983a59aabf7b094a090445e508&token=347564426&lang=zh_CN#rd) | 2023 |
| SFT微调          | [微软 LoRA: 使用万分之一的参数微调你的GPT3模型](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483813&idx=1&sn=c0166d4f023063a0c25fbb761b725298&chksm=cf2b7b5df85cf24b2223c0937534fa2b753eccab34dbef174fd6e9b54ee0d385a523e0f09cb9&token=347564426&lang=zh_CN#rd) | 2023 |
| SFT微调          | [ACL2022 \| 大模型微调哪家好？小孩子才做选择，成年人当然是全都要](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483827&idx=1&sn=dfa120b30ce689ae88f2c42bf76c58f7&chksm=cf2b7b4bf85cf25da24eccbf633047c2c60315600a0d157e58731e8d55c22294d0a1862ccae2&token=347564426&lang=zh_CN#rd) | 2023 |
| SFT微调          | [QLoRA \| 48G内存训练24小时，微调650亿参数模型(4-bit)达到chatgpt99.3%的效果](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483869&idx=1&sn=a2bd2aa21c8056af2ec3cb39ee282fbe&chksm=cf2b7b25f85cf233382c37e64d8d37ccda2f2dc111fee0c3478c74f126737f816d79c05f6d3a&token=347564426&lang=zh_CN#rd) | 2023 |
| LLM落地方案      | [ChatLaw：北大凭什么以13B的基座模型击败恐怖如斯的GPT4？](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484025&idx=1&sn=be46aa4eb439bb4c967e67255f6ad180&chksm=cf2b7881f85cf1977f0de5166d0ef76a0b1484619eb92de3f03e95c57823c13dd7791374dd61&token=347564426&lang=zh_CN#rd) | 2023 |
| Reward Model训练 | [OpenAI \| Let’s Verify Step by Step详细解读](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484337&idx=2&sn=e1fd4baf36d8f9a18cd8e4f049b8d822&chksm=cf2b7949f85cf05f6af461fb5c53a1601d198d58525fc61e0a6ffe7e4912c597768235f28f61&token=347564426&lang=zh_CN#rd) | 2023 |
| Agent            | [TOT(Tree of Thought) ｜ 让GPT-4像人类一样思考](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483949&idx=1&sn=673c778954b767834e1a5155c5d445f5&chksm=cf2b78d5f85cf1c3d6f47088888f19879fd89c9c30fd72848cfdf33ed922a03c327ccb2cca1e&token=347564426&lang=zh_CN#rd) | 2023 |
| Agent            | [ReAct：首次结合Thought和Action提升大模型解决问题的能力](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483986&idx=1&sn=19da7ff92807d4fc1a7459266134ecf8&chksm=cf2b78aaf85cf1bc449984f83468a8736b350e8f3f48597c3e892476b1da3382ea7d7d741fac&token=347564426&lang=zh_CN#rd) | 2023 |
| Agent            | [多agent思想显著提升小模型工具调用能力](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484678&idx=1&sn=184273aba27cb840c22cdd2a62746165&chksm=cf2b7ffef85cf6e82b953561d26786a9e3f84c5898db063e8a8c9b29a53b17ea1fca6ae252c3&token=347564426&lang=zh_CN#rd) | 2024 |
| Agent数据构建    | [ACL2023 \| WebCPM：清华发布中文LFQA 数据集，探索搜索引擎和PLM大模型结合新范式](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483913&idx=1&sn=7bbe8ea2dd5aa0da1d2849a5327839f5&chksm=cf2b78f1f85cf1e77a39d9e745d16cdad012db4d46710ed1873df4026455e40fe807807a5f98&token=347564426&lang=zh_CN#rd) | 2023 |
| 多轮对话         | [LLM4CS：一种利用LLM提升多轮会话检索的效果的方案](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484736&idx=1&sn=31e4a8b8113e31fbee1d9613b3ae48dd&chksm=cf2b7fb8f85cf6aebcdda8be8579bde0ab2546b9c04e52172a49d9600401be9fb7759b9d5d03#rd) | 2024 |
| LLM数据合成      | [EMNLP 2023｜利用LLM合成数据训练模型有哪些坑？](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484648&idx=1&sn=c686b4d4c3b91cfa03204dd8afdda3bb&chksm=cf2b7e10f85cf706a4fce6353631b944076cd858a041560c526aa391b6a7048f51d322a8ffa3&token=347564426&lang=zh_CN#rd) | 2024 |
| LLM数据合成      | [PromptMix: 一种有效的混合数据增强策略将LLM能力迁移到小模型](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484638&idx=3&sn=b0576252f2acd87b5018da1c79d82de9&chksm=cf2b7e26f85cf730ce04206ac4188a774a15a250b0d5968aff02c03a5f396398be1da0f3704f&token=347564426&lang=zh_CN#rd) | 2023 |
| 数据工程         | [符尧：仅靠数据工程我能将LLM的上下文检索能力拓展到128K](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484638&idx=2&sn=70af64fda6bb9d8c97f356cb0485f6c5&chksm=cf2b7e26f85cf7300dcc51d70b78c2ddd75d9bbe3faa4d5ea7a81e4e8de7a5ed4b7753ea5ca4&token=347564426&lang=zh_CN#rd) | 2024 |
| 数据工程         | [符尧：别卷大模型训练了，来卷数据吧！【干货十足】](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484224&idx=1&sn=195522dea690ac08d366ea4fe52268b3&chksm=cf2b79b8f85cf0ae585c350266a20d7505aaef9a96b8e63df34f5e40365b13e6a02c6f6fb876&token=347564426&lang=zh_CN#rd) | 2023 |
| 提示词压缩       | [LLMLingua｜您有一份prompt压缩20倍的方案请查收](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484638&idx=4&sn=5cfd6c0cd2e0a76779347857e844cb0a&chksm=cf2b7e26f85cf7308c2add408920f24150198562f709af890aee6aa40f94602d6edaa0f7d00e&token=347564426&lang=zh_CN#rd) | 2023 |
| 提示词压缩       | [Microsoft \| GPT-4 API太贵了，微软提出省钱大法](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484238&idx=1&sn=87a901221d010ffaf0cdfe7f3a6e3c64&chksm=cf2b79b6f85cf0a02848dd24212512d750601db0845115382dbc00828c891deb1c10c7177399&token=347564426&lang=zh_CN#rd) | 2023 |
| Sora             | [Latte，开源版视频Diffusion Transformer](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484590&idx=1&sn=a2e58bc595d82cd736d2cd01a734e643&chksm=cf2b7e56f85cf7407f93af14d8a2f9cf96ebee2c25fe9477a3bb18702595b25406e6ebd768e2&token=347564426&lang=zh_CN#rd) | 2024 |
| Sora             | [结合OpenAI Sora技术报告来看其能力和技术点](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484519&idx=1&sn=3823f591658d1731518d45edc6ce20d8&chksm=cf2b7e9ff85cf7894084fa443a22b29a8abcbaeaa8b70b500e6d0adf52c32d042985fd77e502&token=347564426&lang=zh_CN#rd) | 2024 |
| LLM知识抽取      | [复旦发布InstructUIE提升大模型信息抽取能力](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484107&idx=1&sn=23cc38347ec79fc25863f6aa9824c952&chksm=cf2b7833f85cf125e9d1b655430eac546b088e7a5a07c54ad2a7a603b4fe0f4d38bec4610363&token=347564426&lang=zh_CN#rd) | 2023 |
| NER              | [EMNLP 2023 TadNER: few-shot Named Entity Recognition](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484425&idx=1&sn=4386705d02e40a45f489d423331e4f23&chksm=cf2b7ef1f85cf7e73b09ea767d776f424088cfb227a6c50e6f2ca42d0faaa5d30bdc9746128d&token=347564426&lang=zh_CN#rd) | 2023 |
| embedding模型    | [Microsoft ｜ 利用LLM本身训练SoTA embedding模型](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247484402&idx=1&sn=1ce3f82aa417a2563173bf7790a3ff77&chksm=cf2b790af85cf01c9b5d37302bcdab116e65405d755fb2ed2a4774b4cfab9b7bb3f3ed70c98f&token=347564426&lang=zh_CN#rd) | 2023 |
| LLM知识注入      | [emnlp 2022:如何将本地图谱知识引入到任务型对话系统中](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483740&idx=1&sn=4581d87836ae33296db6a6b809a6eeb7&chksm=cf2b7ba4f85cf2b2a3e6f56e42f07c57c5d49a4c7826cf681d92f83136330bf7fdb195acd7a4&token=347564426&lang=zh_CN#rd) | 2023 |
| LLM+小模型融合   | [把local小模型当作大语言模型的插件？](https://mp.weixin.qq.com/s?__biz=Mzg3Njk2NTc4Mw==&mid=2247483791&idx=1&sn=5a171649fefdcb51dd13b1278c972462&chksm=cf2b7b77f85cf261cebedc377819d6a7acc0c759aee9546acd6b9c5de8ecc338dadd61ae1baf&token=347564426&lang=zh_CN#rd) | 2023 |

